.TH dupd 1
.SH NAME
dupd \- find duplicate files
.SH SYNOPSIS
.B dupd
.I COMMAND
[OPTIONS]
.SH DESCRIPTION
.B dupd
scans all the files in the given path(s) to find files with duplicate content.
.PP
The sets of duplicate files are not (by default) displayed during a scan.
Instead, the duplicate info is saved into a database which can be queried
with subsequent commands without having to scan all files again.
.SH COMMANDS
As noted in the synopsis, the first argument to dupd must be the command
to run.
The command is one of:
.PP
scan \- scan files looking for duplicates
.PP
report \- show duplicate report from last scan
.PP
file \- check for duplicates of one file
.PP
ls  \- list info about every file
.PP
dups \- list all duplicate files
.PP
uniques \- list all unique files
.PP
refresh \- remove deleted files from the database
.PP
validate \- revalidate all duplicates in database
.PP
rmsh \- create shell script to delete all duplicates (use with care!)
.PP
help \- show brief usage info
.PP
usage \- show this documentation
.PP
man \- show this documentation
.PP
license \- show license info
.PP
version \- show version and exit
.SH OPTIONS
.B scan \- Perform the filesystem scan for duplicates.
.TP
.BR \-p ", " \-\-path " " PATH
Recursively scan the directory tree starting at this path.
The path option can be given multiple times to specify multiple directory trees
to scan.
If no path option is given, the default is to start scanning from the current
directory.
.TP
.BR \-m ", " \-\-minsize " " SIZE
Minimum size (in bytes) to include in scan.
By default all files with 1 byte or more are scanned.
In practice duplicates in files that small are rarely interesting,
so you can speed up the scan by ignoring smaller files.
.TP
.BR \-D ", " \-\-hdd
Select HDD (hard disk drive) scan mode.
By default dupd is optimized for scanning filesystems on SSDs
(solid state drives).
When scanning files located on a HDD (and \fInot\fR in the filesystem cache)
setting this option enables an alternate scan strategy which is faster
on a HDD.
While the HDD mode is generally always faster on HDDs and the SSD mode is
nearly always faster on a SSD, there can be edge case scenarios where this
is not true.
Note in particular that if the file content is in the filesystem cache
(which it \fIwill often be\fR if you are using dupd in the interactive
filesystem exploration mode it was designed to support) then the SSD (default)
mode is usually faster even if the underlying files are stored on a HDD.
.TP
.BR \-\-hidden
Include hidden files (and hidden directories) in the scan.
By default these are not included.
.TP
.BR \-\-db " " PATH
Override the default database file location.
The default is \fB$HOME/.dupd_sqlite\fR.
If you override the path during scan, remember to provide this argument and
the path for subsequent operations so the database can be found.
.TP
.BR \-\-nodb
Do not create a database file.
Duplicate info is sent to stdout instead.
Not recommended, as having the database is required for most of the subsequent
commands documented below.
.TP
.BR \-I ", " \-\-hardlink-is-unique
Consider hard links to the same file content as unique.
By default hard links are listed as duplicates.
See HARD LINKS section below.
Note that if this option is given during scan, it cannot be given during
interactive operations.
.TP
.BR \-\-stats\-file " " FILE
On completion, create (or append to) FILE and save some stats from the run.
These are the same stats as get displayed in verbose mode but are more
suitable for programmatic consumption.
.TP
.BR \-\-file\-count " " COUNT
Estimated maximum number of files to scan.
The default is five million files.
This is only relevant if --hardlink-is-unique is also given.
.PP
.B report \- Display the list of duplicates.
.TP
.BR \-\-cut " " PATHSEG
Remove prefix PATHSEG from the file paths in the report output.
This can reduce clutter in the output text if all the files scanned share
a long identical prefix.
.TP
.BR \-\-minsize " " SIZE
Report only duplicate sets which consume at least this much disk space.
Note this is the total size occupied by all the duplicates in a set,
not their individual file size.
.TP
.BR \-\-format " " NAME
Produce the report in this output format.
NAME is one of text, csv, json.
The default is text.
.PP
Note: The database format generated by scan is not guaranteed to be compatible
with future versions. You should run report (and all the other commands below
which access the database) using the same version of dupd that was used to
generate the database.
.PP
.B file \- Report duplicate status of one file.
.PP
To check whether one given file still has known duplicates use the
file operation.
Note that this does not do a new scan so it will not find new duplicates.
This checks whether the duplicates identified during the previous scan
still exist and verifies (by hash) whether they are still duplicates.
.TP
.BR \-\-file " " PATH
Required: The file to check
.TP
.BR \-\-cut " " PATHSEG
Remove prefix PATHSEG from the file paths in the report output.
.TP
.BR \-\-exclude " " PATH
Ignore any duplicates under PATH when reporting duplicates.
This is useful if you intend to delete the entire tree under PATH,
to make sure you don't delete all copies of the file.
.TP
.BR \-\-hardlink\-is\-unique
Ignore the existence of hard links to the file for the purpose of
considering whether the file is unique.
.PP
.B ls, uniques, dups \- List matching files.
.PP
While the file command checks the duplicate status of a single file,
these commands do the same for all the files in a given directory tree.
.PP
ls \- List all files, show whether they have duplicates or not.
.PP
uniques \- List all unique files.
.PP
dups \- List all files which have known duplicates.
.TP
.BR \-\-path " " PATH
Start from this directory (default is current directory)
.TP
.BR \-\-cut " " PATHSEG
Remove prefix $PATHSEG from the file paths in the output.
.TP
.BR \-\-exclude " " PATH
Ignore any duplicates under PATH when reporting duplicates.
.TP
.BR \-\-hardlink\-is\-unique
Ignore the existence of hard links to the file for the purpose of considering
whether the file is unique.
.PP
.B refresh \- Refreshing the database.
.PP
As you remove duplicate files these are still listed in the dupd database.
Ideally you'd run the scan again to rebuild the database.
Note that re-running the scan after deleting some duplicates can be
very fast because the files are in the cache, so that is the best option.
.PP
However, when dealing with a set of files large enough that they don't fit
in the cache, re-running the scan may take a long time.
For those cases the refresh command offers a much faster alternative.
.PP
The refresh command checks whether all the files in the dupd database still
exist and removes those which do not.
.PP
Be sure to consider the limitations of this approach.
The refresh command \fIdoes not\fR re-verify whether all files listed as
duplicates are still duplicates.
It also, of course, does not detect any new duplicates which may have
appeared since the last scan.
.PP
In summary, if you have only been deleting duplicates since the previous
scan, run the refresh command.
It will prune all the deleted files from the database and will be much
faster than a scan.
However, if you have been adding and/or modifying files since the last
scan, it is best to run a new scan.
.PP
.B validate \- Validating the database.
.PP
The validate operation is primarily for testing but is documented
here as it may be useful if you want to reconfirm that all duplicates
in the database are still truly duplicates.
.PP
In most cases you will be better off re-running the scan operation
instead of using validate.
.PP
Validate is fairly slow as it will fully hash every file in the database.
.PP
.B rmsh - Create shell scrip to remove duplicate files.
.PP
As a policy dupd never modifies the filesystem!
.PP
As a convenience for those times when it is desirable to automatically
remove files, this operation can create a shell script to do so.
The output is a shell script (to stdout) which can you run to delete
your files (if you're feeling lucky).
.PP
Review the generated script carefully to see if it truly does what you want!
.PP
Automated deletion is generally not very useful because it takes human
intervention to decide which of the duplicates is the best one to keep
in each case.
While the content is the same, one of them may have a better file name
and/or location.
.PP
Optionally, the shell script can create either soft or hard links from
each removed file to the copy being kept.
The options are mutually exclusive.
.TP
.BR \-\-link
Create symlinks for deleted files.
.TP
.BR \-\-hardlink
Create hard links for deleted files.
.PP
.B Additional global options
.TP
.BR \-q
Quiet, suppress all output.
.TP
.BR \-v
Verbose mode.
Can be repeated multiple times for ever increasing verbosity.
.TP
.BR \-h
Show brief help summary.
.TP
.BR \-\-db " " PATH
Override the default database file location.
.TP
.BR \-F ", " \-\-hash " " NAME
Specify an different hash function.
This applies to any command which uses content hashing.
NAME is one of: md5 sha1 sha512
.SH HARD LINKS
Are hard links duplicates or not?
The answer depends on "what do you mean by duplicates?" and
"what are you trying to do?"
.PP
If your primary goal for removing duplicates is to save disk space
then it makes sense to ignore hardlinks.
If, on the other hand, your primary goal is to reduce filesystem
clutter then it makes more sense to think of hardlinks as duplicates.
.PP
By default dupd considers hardlinks as duplicates. You can switch this
around with the --hardlink-is-unique option.
This option can be given either during scan or to the interactive
reporting commands (file, ls, uniques, dups).
.SH EXAMPLES
.PP
Scan all files in your home directory and then show the sets of duplicates
found:
.PP
.RS
% dupd scan --path $HOME
.PP
% dupd report
.RE
.PP
Scan all files in the current directory which is on a HDD:
.PP
.RS
% dupd scan --hdd
.RE
.PP
Show duplicate status (duplicate or unique) for all files in docs subdirectory:
.PP
.RS
% dupd ls --path docs
.RE
.PP
I'm about to delete docs/old.doc but want to check one last time that it
is a duplicate and I want to review where those duplicates are:
.PP
.RS
% dupd file --file docs/old.doc -v
.RE
.PP
Read the documentation in the dupd 'docs' directory or online documentation
for more usage examples.
.SH EXIT
dupd exits with status code 0 on success, non-zero on error.
.SH SEE ALSO
.BR sqlite3 (1)

